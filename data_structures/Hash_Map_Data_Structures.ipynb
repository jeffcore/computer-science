{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple hash function  \n",
    "ord(character) to determine ASCII value of a particular character e.g. ord('a') will return 97  \n",
    "This function is bad it has bad collision  \n",
    "For starters, it will return the same value for abcd and bcda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n",
      "394\n"
     ]
    }
   ],
   "source": [
    "def hash_function(string):\n",
    "    hash_code = 0\n",
    "    for character in string:\n",
    "        hash_code += ord(character)\n",
    "    return hash_code\n",
    "\n",
    "hash_code_1 = hash_function(\"abcd\")\n",
    "print(hash_code_1)\n",
    "hash_code_1 = hash_function(\"bcda\")\n",
    "print(hash_code_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash functions for string\n",
    "For a string, say `abcde`, a very effective function is treating this as number of prime number base `p`. \n",
    "Let's elaborate this statement. \n",
    "\n",
    "For a number, say `578`, we can represent this number in base 10 number system as $$5*10^2 + 7*10^1 + 8*10^0$$\n",
    "\n",
    "Similarly, we can treat `abcde` as $$a * p^4 + b * p^3 + c * p^2 + d * p^1 + e * p^0$$\n",
    "\n",
    "Here, we replace each character with its corresponding ASCII value. \n",
    "\n",
    "A lot of research goes into figuring out good hash functions and this hash function is one of the most popular functions used for strings. We use prime numbers because the provide a good distribution. The most common prime numbers used for this function are 31 and 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5204554\n",
      "5054002\n"
     ]
    }
   ],
   "source": [
    "class HashMap:\n",
    "    \n",
    "    def __init__(self, initial_size=10):\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.p = 37\n",
    "        self.num_entries = 0\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        pass\n",
    "    \n",
    "    def get(self, key):\n",
    "        pass\n",
    "    \n",
    "    def get_bucket_index(self, key):\n",
    "        return self.get_hash_code(key)\n",
    "    \n",
    "    def get_hash_code(self, key):\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        current_coefficient = 1\n",
    "        hash_code = 0\n",
    "        for character in key:\n",
    "            hash_code += ord(character) * current_coefficient\n",
    "            current_coefficient *= self.p\n",
    "            current_coefficient = current_coefficient\n",
    "\n",
    "        return hash_code\n",
    "    \n",
    "hash_map = HashMap()\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"abcd\")\n",
    "print(bucket_index)\n",
    "\n",
    "hash_map = HashMap()\n",
    "\n",
    "bucket_index = hash_map.get_bucket_index(\"bcda\")\n",
    "print(bucket_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collision Handling\n",
    "\n",
    "As discussed earlier, when two different inputs produce the same output, then we have a collision. Our implementation of `get_hash_code()` function is satisfactory. However, because we are using compression function, we are prone to collisions. \n",
    "\n",
    "Consider the following scenario. \n",
    "\n",
    "We have a bucket array of length 10 and we get two different hash codes for two different inputs, say 355, and 1095. Even though the hash codes are different in this case, the bucket index will be same because of the way we have implemented our compression function. Such scenarios where multiple entries want to go to the same bucket are very common. So, we introduce some logic to handle collisions.\n",
    "\n",
    "There are two popular ways in which we handle collisions.\n",
    "\n",
    "1. Closed Addressing or Separate chaining\n",
    "2. Open Addressing\n",
    "\n",
    "1. Closed addressing is a clever technique where we use the same bucket to store multiple objects. The bucket in this case will store a linked list of key-value pairs. Every bucket has it's own separate chain of linked list nodes.\n",
    "\n",
    "2. In open addressing, we do the following:\n",
    "    * If, after getting the bucket index,  the bucket is empty, we store the object in that particular bucket\n",
    "    \n",
    "    * If the bucket is not empty, we find an alternate bucket index by using another function which modifies the current hash code to give a new code\n",
    "    \n",
    "\n",
    "Separate chaining is a simple and effective technique to handle collisions and that is what we discuss here. \n",
    "\n",
    "Implement the `put` and `get` function using the idea of separate chaining. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression Function\n",
    "\n",
    "We now have a good hash function which will return unique values for unique objects. But let's look at the values. These are huge. We cannot create such large arrays. So we use another function - `compression function` to compress these values so as to create arrays of reasonable sizes. \n",
    "\n",
    "A very simple, good, and effective compression function can be ` mod len(array)`. The `modulo operator %` returns the remainder of one number when divided by other. \n",
    "\n",
    "So, if we have an array of size 10, we can be sure that modulo of any number with 10 will be less than 10, allowing it to fit into our bucket array.\n",
    "\n",
    "Because of how modulo operator works, instead of creating a new function, we can write the logic for compression function in our `get_hash_code()` function itself.\n",
    "\n",
    "https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/modular-multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n",
      "size: 4\n"
     ]
    }
   ],
   "source": [
    "class LinkedListNode:\n",
    "    \n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class HashMap:\n",
    "    \n",
    "    def __init__(self, initial_size = 10):\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.p = 31\n",
    "        self.num_entries = 0\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "\n",
    "        new_node = LinkedListNode(key, value)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "\n",
    "        # check if key is already present in the map, and update it's value\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                head.value = value\n",
    "                return\n",
    "            head = head.next\n",
    "\n",
    "        # key not found in the chain --> create a new entry and place it at the head of the chain\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        new_node.next = head\n",
    "        self.bucket_array[bucket_index] = new_node\n",
    "        self.num_entries += 1\n",
    "        \n",
    "    def get(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                return head.value\n",
    "            head = head.next\n",
    "        return None\n",
    "        \n",
    "    def get_bucket_index(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        return bucket_index\n",
    "    \n",
    "    def get_hash_code(self, key):\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        current_coefficient = 1\n",
    "        hash_code = 0\n",
    "        for character in key:\n",
    "            hash_code += ord(character) * current_coefficient\n",
    "            hash_code = hash_code % num_buckets                       # compress hash_code\n",
    "            current_coefficient *= self.p\n",
    "            current_coefficient = current_coefficient % num_buckets   # compress coefficient\n",
    "\n",
    "        return hash_code % num_buckets                                # one last compression before returning\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "\n",
    "\n",
    "hash_map = HashMap()\n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)\n",
    "hash_map.put(\"neo\", 11)\n",
    "\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "print(\"size: {}\".format(hash_map.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used arrays to implement our hashmaps because arrays offer $O(1)$ time complexity for both put and get operations. \n",
    "\n",
    "*Note: in case of arrays put is simply `arr[i] = 5` and get is `height = arr[5]`*\n",
    "\n",
    "#### 1. Put Operation\n",
    "\n",
    "* In the put operation, we first figure out the bucket index. Calculating the hash code to figure out the bucket index takes some time.\n",
    "\n",
    "* After that, we go to the bucket index and in the worst case we traverse the linked list to find out if the key is already present or not. This also takes some time.\n",
    "\n",
    "To analyze the time complexity for any algorithm as a function of the input size `n`, we first have to determine what our input is. In this case, we are putting and gettin key value pairs. So, these entries i.e. key-value pairs are our input. Therefore, our `n` is number of such key-value pair entries.\n",
    "\n",
    "*Note: time complexity is always determined in terms of input size and not the actual amount of work that is being done independent of input size. That \"independent amount of work\" will be constant for every input size so we disregard that.*\n",
    "\n",
    "\n",
    "* In case of our hash function, the computation time for hash code depends on the size of each string. Compared to number of entries (which we always consider to be very high e.g. in the order of $10^5$) the length of each string can be considered to be very small. Also, most of the strings will be around the same size when compared to this high number of entries. Hence, we can ignore the hash computation time in our analysis of time complexity.\n",
    "\n",
    "\n",
    "* Now, the entire time complexity essentialy depends on the linked list traversal. In the worst case, all entries would go to the same bucket index and our linked list at that index would be huge. Therefore, the time complexity in that scenario would be $O(n)$. However, hash functions are wisely chosen so that this does not happen. \n",
    "\n",
    "`On average, the distribution of entries is such that if we have n entries and b buckets, then each bucket does not have more than n/b key-value pair entries.` \n",
    "\n",
    "Therefore, because of our choice of hash functions, we can assume that the time complexity is $O(\\dfrac{n}{b})$.\n",
    "This number which determines the `load` on our bucket array `n/b` is known as load factor. \n",
    "\n",
    "Generally, we try to keep our load factor around or less than 0.7. This essentially means that if we have a bucket array of size 10, then the number of key-value pair entries will not be more than 7.\n",
    "\n",
    "**What happens when we get more entries and the value of our load factor crosses 0.7?**\n",
    "\n",
    "In that scenario, we must increase the size of our bucket array. Also, we must recalculate the bucket index for each entry in the hashn map.\n",
    "\n",
    "*Note: the hash code for each key present in the bucket array would still be the same. However, because of the compression function, the bucket index will change.* \n",
    "\n",
    "Therefore, we need to `rehash` all the entries in our hash map. This is known as `Rehashing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n",
      "size: 4\n",
      "\n",
      "size: 4\n",
      "one: 1\n",
      "neo: 11\n",
      "three: 3\n",
      "size: 4\n",
      "None\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "class LinkedListNode:\n",
    "    \n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "class HashMap:\n",
    "    \n",
    "    def __init__(self, initial_size = 15):\n",
    "        self.bucket_array = [None for _ in range(initial_size)]\n",
    "        self.p = 31\n",
    "        self.num_entries = 0\n",
    "        self.load_factor = 0.7\n",
    "        \n",
    "    def put(self, key, value):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "\n",
    "        new_node = LinkedListNode(key, value)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "\n",
    "        # check if key is already present in the map, and update it's value\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                head.value = value\n",
    "                return\n",
    "            head = head.next\n",
    "\n",
    "        # key not found in the chain --> create a new entry and place it at the head of the chain\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        new_node.next = head\n",
    "        self.bucket_array[bucket_index] = new_node\n",
    "        self.num_entries += 1\n",
    "        \n",
    "        # check for load factor\n",
    "        current_load_factor = self.num_entries / len(self.bucket_array)\n",
    "        if current_load_factor > self.load_factor:\n",
    "            self.num_entries = 0\n",
    "            self._rehash()\n",
    "        \n",
    "    def get(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                return head.value\n",
    "            head = head.next\n",
    "        return None\n",
    "        \n",
    "    def get_bucket_index(self, key):\n",
    "        bucket_index = self.get_hash_code(key)\n",
    "        return bucket_index\n",
    "    \n",
    "    def get_hash_code(self, key):\n",
    "        key = str(key)\n",
    "        num_buckets = len(self.bucket_array)\n",
    "        current_coefficient = 1\n",
    "        hash_code = 0\n",
    "        for character in key:\n",
    "            hash_code += ord(character) * current_coefficient\n",
    "            hash_code = hash_code % num_buckets                       # compress hash_code\n",
    "            current_coefficient *= self.p\n",
    "            current_coefficient = current_coefficient % num_buckets   # compress coefficient\n",
    "        return hash_code % num_buckets                                # one last compression before returning\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num_entries\n",
    "\n",
    "    def _rehash(self):\n",
    "        old_num_buckets = len(self.bucket_array)\n",
    "        old_bucket_array = self.bucket_array\n",
    "        num_buckets = 2 * old_num_buckets\n",
    "        self.bucket_array = [None for _ in range(num_buckets)]\n",
    "\n",
    "        for head in old_bucket_array:\n",
    "            while head is not None:\n",
    "                key = head.key\n",
    "                value = head.value\n",
    "                self.put(key, value)         # we can use our put() method to rehash\n",
    "                head = head.next\n",
    "\n",
    "    def delete(self, key):\n",
    "        bucket_index = self.get_bucket_index(key)\n",
    "        head = self.bucket_array[bucket_index]\n",
    "\n",
    "        previous = None\n",
    "        while head is not None:\n",
    "            if head.key == key:\n",
    "                if previous is None:\n",
    "                    self.bucket_array[bucket_index] = head.next\n",
    "                else:\n",
    "                    previous.next = head.next\n",
    "                self.num_entries -= 1\n",
    "                return\n",
    "            else:\n",
    "                previous = head\n",
    "                head = head.next\n",
    "                \n",
    "                \n",
    "## rehash tests               \n",
    "hash_map = HashMap(7)\n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)\n",
    "hash_map.put(\"neo\", 11)\n",
    "\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "# delete tests\n",
    "hash_map = HashMap(7)\n",
    "\n",
    "hash_map.put(\"one\", 1)\n",
    "hash_map.put(\"two\", 2)\n",
    "hash_map.put(\"three\", 3)\n",
    "hash_map.put(\"neo\", 11)\n",
    "\n",
    "print(\"\\nsize: {}\".format(hash_map.size()))\n",
    "\n",
    "\n",
    "print(\"one: {}\".format(hash_map.get(\"one\")))\n",
    "print(\"neo: {}\".format(hash_map.get(\"neo\")))\n",
    "print(\"three: {}\".format(hash_map.get(\"three\")))\n",
    "print(\"size: {}\".format(hash_map.size()))\n",
    "\n",
    "hash_map.delete(\"one\")\n",
    "\n",
    "print(hash_map.get(\"one\"))\n",
    "print(hash_map.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple string hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8568\n",
      "-1\n",
      "8568\n",
      "8568\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Write a HashTable class that stores strings\n",
    "in a hash table, where keys are calculated\n",
    "using the first two letters of the string.\"\"\"\n",
    "\n",
    "class HashTable(object):\n",
    "    def __init__(self):\n",
    "        self.table = [None]*10000\n",
    "\n",
    "    def store(self, string):\n",
    "        \"\"\"TODO: Input a string that's stored in \n",
    "        the table.\"\"\"\n",
    "        hash_value = self.calculate_hash_value(string)\n",
    "        \n",
    "        if self.table[hash_value] != None:\n",
    "            self.table[hash_value].append(hash_value)\n",
    "        else:\n",
    "            self.table[hash_value] = [string]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def lookup(self, string):\n",
    "        \"\"\"TODO: Return the hash value if the\n",
    "        string is already in the table.\n",
    "        Return -1 otherwise.\"\"\"\n",
    "        hash_value = self.calculate_hash_value(string)\n",
    "        if self.table[hash_value]:\n",
    "            return hash_value\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def calculate_hash_value(self, string):\n",
    "        \"\"\"TODO: Helper function to calulate a\n",
    "        hash value from a string.\"\"\"\n",
    "        \n",
    "        hash_value = ord(string[0]) * 100 + ord(string[1]) \n",
    "        \n",
    "        return hash_value\n",
    "\n",
    "# Setup\n",
    "hash_table = HashTable()\n",
    "\n",
    "# Test calculate_hash_value\n",
    "# Should be 8568\n",
    "print (hash_table.calculate_hash_value('UDACITY'))\n",
    "\n",
    "# Test lookup edge case\n",
    "# Should be -1\n",
    "print (hash_table.lookup('UDACITY'))\n",
    "\n",
    "# Test store\n",
    "hash_table.store('UDACITY')\n",
    "# Should be 8568\n",
    "print (hash_table.lookup('UDACITY'))\n",
    "\n",
    "# Test store edge case\n",
    "hash_table.store('UDACIOUS')\n",
    "# Should be 8568\n",
    "print (hash_table.lookup('UDACIOUS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashMaps\n",
    "use python dictionaries they are implemented using hash tables, sets are also implemented this way\n",
    "\n",
    "## Hash Tables\n",
    "you come up with your own unique hash ast the key , then store arrays of data in each bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8568\n",
      "-1\n",
      "8568\n",
      "8568\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "class HashTable(object):\n",
    "    def __init__(self):\n",
    "        self.table = [None]*10000\n",
    "\n",
    "    def store(self, string):\n",
    "        \"\"\"Input a string that's stored in \n",
    "        the table.\"\"\"\n",
    "        hash_value = self.calculate_hash_value(string)\n",
    "        if hash_value != -1: # no ideal why this was here hash would never calc to -1\n",
    "            if self.table[hash_value]:\n",
    "                self.table[hash_value].append(string)                \n",
    "            else:\n",
    "                self.table[hash_value] = [string]\n",
    "\n",
    "\n",
    "    def lookup(self, string):\n",
    "        \"\"\"Return the hash value if the\n",
    "        string is already in the table.\n",
    "        Return -1 otherwise.\"\"\"\n",
    "        hash_value = self.calculate_hash_value(string)\n",
    "        if hash_value != -1:\n",
    "            if self.table[hash_value]:\n",
    "                if string in self.table[hash_value]:                   \n",
    "                    return hash_value\n",
    "\n",
    "        return -1\n",
    "\n",
    "    def calculate_hash_value(self, string):\n",
    "        \"\"\"Helper function to calulate a\n",
    "        hash value from a string.\n",
    "        Hash Value = (ASCII Value of First Letter * 100) + ASCII Value of Second Letter\"\"\"\n",
    "        hash_value = ord(string[0]) * 100 + ord(string[1])\n",
    "        return hash_value\n",
    "\n",
    "# Setup\n",
    "hash_table = HashTable()\n",
    "\n",
    "# Test calculate_hash_value\n",
    "# Should be 8568\n",
    "print(hash_table.calculate_hash_value('UDACITY'))\n",
    "\n",
    "# Test lookup edge case\n",
    "# Should be -1\n",
    "print(hash_table.lookup('UDACITY'))\n",
    "\n",
    "# Test store\n",
    "hash_table.store('UDACITY')\n",
    "# Should be 8568\n",
    "print(hash_table.lookup('UDACITY'))\n",
    "\n",
    "# Test store edge case\n",
    "hash_table.store('UDACIOUS')\n",
    "# Should be 8568\n",
    "print(hash_table.lookup('UDACIOUS'))\n",
    "print(hash_table.lookup('UDAcS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
